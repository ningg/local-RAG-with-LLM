import faiss
import numpy as np
import requests
import textwrap
import json
import os
import glob
import time
from pathlib import Path

# ========= 1. Ollama API å°è£… =========
OLLAMA_API = "http://localhost:11434/api"

def ollama_embed(text, model="deepseek-r1:8b"):
    """è°ƒç”¨ Ollama embedding æ¥å£"""
    resp = requests.post(f"{OLLAMA_API}/embed", json={"model": model, "input": text})
    data = resp.json()
    return np.array(data["embeddings"][0], dtype="float32")

def ollama_chat(prompt, model="deepseek-r1:8b"):
    """è°ƒç”¨ Ollama Chat æ¥å£"""
    payload = {
        "model": model,
        "prompt": prompt,
        "stream": False
    }
    resp = requests.post(f"{OLLAMA_API}/generate", json=payload)
    data = resp.json()
    return data["response"].strip()

# ========= 2. ä»ç›®å½•è¯»å–Markdownæ–‡ä»¶ =========
def load_md_files_from_directory(directory_path):
    """ä»æŒ‡å®šç›®å½•è¯»å–æ‰€æœ‰.mdæ–‡ä»¶"""
    md_files = []
    md_path = Path(directory_path)
    
    if not md_path.exists():
        print(f"ç›®å½•ä¸å­˜åœ¨: {directory_path}")
        return []
    
    if not md_path.is_dir():
        print(f"è·¯å¾„ä¸æ˜¯ç›®å½•: {directory_path}")
        return []
    
    # æŸ¥æ‰¾æ‰€æœ‰.mdæ–‡ä»¶
    md_pattern = md_path / "**/*.md"
    md_files = list(md_path.glob("**/*.md"))
    
    if not md_files:
        print(f"åœ¨ç›®å½• {directory_path} ä¸­æœªæ‰¾åˆ°.mdæ–‡ä»¶")
        return []
    
    print(f"æ‰¾åˆ° {len(md_files)} ä¸ªMarkdownæ–‡ä»¶:")
    for file_path in md_files:
        print(f"  - {file_path}")
    
    return md_files

def read_md_file(file_path):
    """è¯»å–å•ä¸ªMarkdownæ–‡ä»¶å†…å®¹"""
    try:
        with open(file_path, 'r', encoding='utf-8') as f:
            content = f.read()
        return content
    except Exception as e:
        print(f"è¯»å–æ–‡ä»¶ {file_path} å¤±è´¥: {e}")
        return None

def load_all_md_content(directory_path):
    """åŠ è½½ç›®å½•ä¸‹æ‰€æœ‰Markdownæ–‡ä»¶çš„å†…å®¹"""
    md_files = load_md_files_from_directory(directory_path)
    
    if not md_files:
        return ""
    
    all_content = []
    total_files = len(md_files)
    successful_files = 0
    
    # å®šä¹‰è¦åŒ…å«çš„å¹´ä»½å‰ç¼€
    valid_years = ['2025']
    
    for file_path in md_files:
        # æ£€æŸ¥æ–‡ä»¶åæ˜¯å¦ä»¥æŒ‡å®šå¹´ä»½å¼€å¤´
        file_name = file_path.name
        if not any(file_name.startswith(year) for year in valid_years):
            print(f"è·³è¿‡æ–‡ä»¶: {file_name} (å¹´ä»½ä¸ç¬¦åˆè¦æ±‚)")
            continue
            
        content = read_md_file(file_path)
        if content:
            # æ·»åŠ æ–‡ä»¶æ ‡è¯†ä¿¡æ¯
            file_info = f"\n\n--- æ–‡ä»¶: {file_name} ---\n\n"
            all_content.append(file_info + content)
            successful_files += 1
            print(f"âœ… æˆåŠŸè¯»å–: {file_name} ({len(content)} å­—ç¬¦)")
        else:
            print(f"âŒ è·³è¿‡æ–‡ä»¶: {file_name} (è¯»å–å¤±è´¥)")
    
    print(f"\næ€»è®¡: {successful_files}/{total_files} ä¸ªæ–‡ä»¶è¯»å–æˆåŠŸ")
    
    return "\n".join(all_content)

# é…ç½®æ–‡æ¡£ç›®å½•è·¯å¾„
DOCS_DIR = "/Users/guoning/ningg/github/ningg.github.com/_posts/blog"  # è¯·æ›¿æ¢ä¸ºå®é™…çš„ç›®å½•è·¯å¾„

# åŠ è½½æ‰€æœ‰Markdownæ–‡ä»¶å†…å®¹
print("=== å¼€å§‹åŠ è½½Markdownæ–‡ä»¶ ===")
start_time = time.time()

docs_text = load_all_md_content(DOCS_DIR)
if not docs_text:
    print("æœªè·å–åˆ°ä»»ä½•æ–‡æ¡£å†…å®¹ï¼Œç¨‹åºé€€å‡º")
    exit(1)

load_time = time.time() - start_time
print(f"âœ… æ–‡ä»¶åŠ è½½å®Œæˆï¼Œè€—æ—¶: {load_time:.2f}ç§’")
print(f"æ€»æ–‡æ¡£å†…å®¹é•¿åº¦: {len(docs_text)} å­—ç¬¦")

# æ˜¾ç¤ºå†…å®¹é¢„è§ˆ
print("\n=== æ–‡æ¡£å†…å®¹é¢„è§ˆ ===")
preview_length = min(800, len(docs_text))
print(docs_text[:preview_length] + ("..." if len(docs_text) > preview_length else ""))
print("=" * 60)

def split_into_chunks(text, chunk_size=500):
    """å°†æ–‡æœ¬åˆ‡åˆ†æˆæŒ‡å®šå¤§å°çš„å—"""
    text = text.replace("\n", " ")
    return textwrap.wrap(text, chunk_size)

# å°†æ–‡æ¡£æ–‡æœ¬åˆ‡åˆ†æˆå—
print("=== å¼€å§‹æ–‡æœ¬åˆ‡åˆ† ===")
chunk_start_time = time.time()

chunks = split_into_chunks(docs_text)
chunk_time = time.time() - chunk_start_time
print(f"âœ… æ–‡æœ¬åˆ‡åˆ†å®Œæˆï¼Œè€—æ—¶: {chunk_time:.2f}ç§’")
print(f"æ–‡æ¡£åˆ‡åˆ†ä¸º {len(chunks)} ä¸ªå—")

# ç”Ÿæˆæ–‡æ¡£å—çš„å‘é‡è¡¨ç¤º
print("=== å¼€å§‹ç”Ÿæˆå‘é‡è¡¨ç¤º ===")
embedding_start_time = time.time()
chunk_embeddings = []
for i, chunk in enumerate(chunks):
    if i % 10 == 0:  # æ¯10ä¸ªå—æ˜¾ç¤ºä¸€æ¬¡è¿›åº¦
        print(f"å¤„ç†è¿›åº¦: {i+1}/{len(chunks)}")
    emb = ollama_embed(chunk)
    chunk_embeddings.append(emb)

embedding_time = time.time() - embedding_start_time
print(f"âœ… å‘é‡ç”Ÿæˆå®Œæˆï¼Œè€—æ—¶: {embedding_time:.2f}ç§’")
print(f"å¹³å‡æ¯ä¸ªå—è€—æ—¶: {embedding_time/len(chunks):.3f}ç§’")

# ========= 3. æ„å»ºå‘é‡åº“ =========
print("=== å¼€å§‹æ„å»ºå‘é‡åº“ ===")
index_start_time = time.time()

dim = len(ollama_embed("æµ‹è¯•"))  # å‘é‡ç»´åº¦
index = faiss.IndexFlatL2(dim) 
index.add(np.array(chunk_embeddings))

index_time = time.time() - index_start_time
print(f"âœ… å‘é‡åº“æ„å»ºå®Œæˆï¼Œè€—æ—¶: {index_time:.2f}ç§’")
print(f"å‘é‡åº“ç»´åº¦: {dim}, å‘é‡æ•°é‡: {len(chunk_embeddings)}")

# ========= 4. æ£€ç´¢ =========
def retrieve_chunks(query, top_k=3):
    """æ£€ç´¢ä¸æŸ¥è¯¢æœ€ç›¸å…³çš„æ–‡æ¡£å—"""
    query_emb = ollama_embed(query)
    D, I = index.search(np.array([query_emb]), top_k)
    return [chunks[i] for i in I[0]]

# ========= 5. ç”Ÿæˆç­”æ¡ˆ =========
def answer_query(query):
    """æ ¹æ®æ£€ç´¢åˆ°çš„æ–‡æ¡£å—ç”Ÿæˆç­”æ¡ˆ"""
    start_time = time.time()
    
    # æ£€ç´¢ç›¸å…³æ–‡æ¡£å—
    retrieve_start = time.time()
    retrieved = retrieve_chunks(query, top_k=3)
    retrieve_time = time.time() - retrieve_start
    
    context = "\n".join(retrieved)

    # ç”Ÿæˆç­”æ¡ˆ
    chat_start = time.time()
    prompt = f"""
ä½ æ˜¯ä¸€ä¸ªæ–‡æ¡£é—®ç­”åŠ©æ‰‹ã€‚
åªèƒ½æ ¹æ®ä»¥ä¸‹æä¾›çš„æ–‡æ¡£å†…å®¹å›ç­”ç”¨æˆ·é—®é¢˜ï¼Œ
å¦‚æœæ‰¾ä¸åˆ°ï¼Œè¯·å›ç­”ï¼š"æŠ±æ­‰ï¼Œæ–‡æ¡£ä¸­æ²¡æœ‰ç›¸å…³å†…å®¹"ã€‚

æ–‡æ¡£å†…å®¹:
{context}

ç”¨æˆ·é—®é¢˜: {query}
è¯·ç”¨è‡ªç„¶è¯­è¨€æ€»ç»“æˆ–æ”¹å†™å›ç­”ï¼š
"""
    answer = ollama_chat(prompt)
    chat_time = time.time() - chat_start
    
    total_time = time.time() - start_time
    
    print(f"â±ï¸  æ£€ç´¢è€—æ—¶: {retrieve_time:.2f}ç§’, ç”Ÿæˆè€—æ—¶: {chat_time:.2f}ç§’, æ€»è®¡: {total_time:.2f}ç§’")
    
    return answer

# ========= 6. äº¤äº’å¼é—®ç­” =========
def interactive_qa():
    """äº¤äº’å¼é—®ç­”æ¨¡å¼"""
    print("\n" + "="*60)
    print("è¿›å…¥äº¤äº’å¼é—®ç­”æ¨¡å¼ (è¾“å…¥ 'quit' æˆ– 'exit' é€€å‡º)")
    print("="*60)
    
    while True:
        try:
            query = input("\nè¯·è¾“å…¥æ‚¨çš„é—®é¢˜: ").strip()
            
            if query.lower() in ['quit', 'exit', 'é€€å‡º', 'q']:
                print("æ„Ÿè°¢ä½¿ç”¨ï¼Œå†è§ï¼")
                break
            
            if not query:
                print("è¯·è¾“å…¥æœ‰æ•ˆçš„é—®é¢˜")
                continue
            
            print(f"\næ­£åœ¨å¤„ç†é—®é¢˜: {query}")
            answer = answer_query(query)
            print(f"ç­”æ¡ˆ: {answer}")
            
        except KeyboardInterrupt:
            print("\n\nç¨‹åºè¢«ä¸­æ–­ï¼Œé€€å‡º...")
            break
        except Exception as e:
            print(f"å¤„ç†é—®é¢˜æ—¶å‡ºé”™: {e}")

# ========= 7. ä¸»ç¨‹åº =========
if __name__ == "__main__":
    program_start_time = time.time()
    
    print("=== Markdownæ–‡ä»¶RAGé—®ç­”ç³»ç»Ÿ ===")
    print(f"æ–‡æ¡£ç›®å½•: {DOCS_DIR}")
    
    # æµ‹è¯•é—®ç­”
    test_question = "è¯·ä»‹ç»ä¸€ä¸‹è¿™äº›æ–‡æ¡£çš„ä¸»è¦å†…å®¹"
    print(f"\næµ‹è¯•é—®é¢˜: {test_question}")
    print("æ­£åœ¨ç”Ÿæˆç­”æ¡ˆ...")
    
    try:
        answer = answer_query(test_question)
        print(f"æµ‹è¯•ç­”æ¡ˆ: {answer}")
    except Exception as e:
        print(f"æµ‹è¯•é—®ç­”å¤±è´¥: {e}")
    
    # è®¡ç®—åˆå§‹åŒ–æ€»æ—¶é—´
    init_time = time.time() - program_start_time
    print(f"\nğŸ¯ ç³»ç»Ÿåˆå§‹åŒ–æ€»è€—æ—¶: {init_time:.2f}ç§’")
    print("=" * 60)
    
    # è¿›å…¥äº¤äº’å¼é—®ç­”æ¨¡å¼
    interactive_qa()
